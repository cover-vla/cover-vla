{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(520, 7, 1, 32064)\n",
      "(520, 7)\n",
      "(520, 224, 224, 3)\n",
      "[-14.6875 -17.375  -13.375  ... -15.75   -15.8125 -17.375 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "root_folder = '../rollouts'\n",
    "folder_name = \"2025_01_16-19_57_24--episode=1--success=False--task=put_both_the_alphabet_soup_and_the_tomato_sauce_in_the_basket\"\n",
    "action_prob_path = f\"{root_folder}/action_probs/2025_01_16/{folder_name}.npz\"\n",
    "action_path = f\"{root_folder}/actions/2025_01_16/{folder_name}.npz\"\n",
    "image_path = f\"{root_folder}/images/2025_01_16/{folder_name}.npz\"\n",
    "\n",
    "action_probs = np.load(action_prob_path)['put_both_the_alphabet_soup_and_the_tomato_sauce_in_the_basket']\n",
    "actions = np.load(action_path)['put_both_the_alphabet_soup_and_the_tomato_sauce_in_the_basket']\n",
    "images = np.load(image_path)['put_both_the_alphabet_soup_and_the_tomato_sauce_in_the_basket']\n",
    "\n",
    "print(action_probs.shape)\n",
    "print (actions.shape)\n",
    "print(images.shape)\n",
    "print (action_probs[0,0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words saved to /home/xilun/LIBERO/libero/datasets/unique_words.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the datasets folder path\n",
    "datasets_folder = \"/home/xilun/LIBERO/libero/datasets\"  # Change this to your actual path\n",
    "\n",
    "# Set to store unique words\n",
    "unique_words = set()\n",
    "\n",
    "# Iterate through all subfolders and files in datasets/\n",
    "for root, _, files in os.walk(datasets_folder):\n",
    "    for file in files:\n",
    "        # Ignore hidden or non-relevant files\n",
    "        if file.startswith(\".\") or not file.endswith(\".hdf5\"):  # Adjust extension if needed\n",
    "            continue\n",
    "        \n",
    "        # Remove file extension and split by \"_\"\n",
    "        words = file.rsplit(\".\", 1)[0].split(\"_\")\n",
    "        \n",
    "        # Add words to the set\n",
    "        unique_words.update(words)\n",
    "\n",
    "# Convert set to sorted list\n",
    "unique_words_list = sorted(unique_words)\n",
    "\n",
    "# Save unique words to a JSON file\n",
    "output_json = os.path.join(datasets_folder, \"unique_words.json\")\n",
    "with open(output_json, \"w\") as json_file:\n",
    "    json.dump(unique_words_list, json_file, indent=4)\n",
    "\n",
    "print(f\"Unique words saved to {output_json}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
